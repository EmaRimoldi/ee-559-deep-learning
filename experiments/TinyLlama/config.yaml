# config.yaml

task_type: "SEQ_CLS"
r: 2
lora_alpha: 16
lora_dropout: 0.05
target_modules:
  - "k_proj"
  - "v_proj"

output_dir: "experiments/TinyLlama/checkpoints"
learning_rate: 0.001
num_train_epochs: 3
weight_decay: 0.01
eval_strategy: "epoch"
save_strategy: "epoch"
load_best_model_at_end: true
logging_dir: "experiments/TinyLlama/logs"
logging_steps: 100
model_checkpoint: 'PY007/TinyLlama-1.1B-step-50K-105b'
model_name: 'TinyLlama'

