![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![PyTorch](https://img.shields.io/badge/pytorch-%EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Hugging Face](https://img.shields.io/badge/Hugging%20Face-FF6A00?style=for-the-badge&logo=huggingface&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-%23150458?style=for-the-badge&logo=pandas&logoColor=white)



# HateLens: Tiny LLMs for Efficient & Interpretable Hate Speech Detection

## ğŸ‘¥ Group 48
| Student's name | SCIPER |
| -------------- | ------ |
| [Vittoria Meroni](https://github.com/vittoriameroni) | 386722 |
| [Emanuele Rimoldi](https://github.com/EmaRimoldi) | 377013 |
| [Simone Vicentini](https://github.com/SimoVice/) | 378204 |

## ğŸ“„ Deliverables
- [Screencase](https://drive.google.com/file/d/1CwCQC62-vEC8ymORb9-4itADhIKu_CwO/view?usp=sharing) (download for better quality)

## ğŸŒ Description

HateLens is a lightweight, transparent pipeline designed to detect and explain hate speech on social platforms. By combining the contextual power of decoder-only TinyLLMs with parameter-efficient fine-tuning and post-hoc explainability, HateLens strikes a balance between high accuracy and minimal computational overhead.

Specifically, HateLens:

- **Leverages TinyLLMs**: Fine-tunes compact, decoder-only language models via Low-Rank Adaptation (LoRA), updating less than 0.05% of parameters to keep memory footprint and inference time low.
- **Ensures Interpretability**: Integrates Local Interpretable Model-agnostic Explanations (LIME) to provide token-level attributions both before and after adaptation, making every classification decision transparent.
- **Maintains Generative Capabilities**: Preserves the base modelâ€™s generative strengths by storing only a small set of differential weights, allowing seamless reuse for other tasks.
- **Delivers State-of-the-Art Performance**: On the DynaHate benchmark, our best TinyLLM achieves over 80% accuracyâ€”an improvement of more than 25% compared to its pre-adaptation baseline.

With HateLens, researchers and practitioners gain a fast, reliable, and explainable tool to curb the spread of hateful content without sacrificing efficiency or clarity. Perfect for deployment on edge devices, real-time moderation systems, and research environments where both performance and transparency matter.  

## Data

### ğŸ“Š AirBase

Maintained by the European Environment Agency (EEA), [AirBase](https://www.eea.europa.eu/en/datahub/datahubitem-view/778ef9f5-6293-4846-badd-56a29c70880d?activeAccordion=1087599) is our primary data source. It compiles air quality measurements from EU Member States, EEA countries, and partner nations. The dataset includes a multiyear time series of pollutant levels, along with metadata on monitoring networks and stations.

ğŸ§ª Scripts & Notebooks:
- `data/download_eea_air_quality_data.py` â€” Python script to download AirBase data.
- `analysis/eea_air_quality_data_eda.ipynb` â€” Initial exploratory data analysis of the dataset.
- `preprocess/*` â€” Scripts used to aggregate and clean the data into the final file `air_quality_data.json`, which is consumed by the web app. 

ğŸ”— Useful Links:
- ğŸ“„ [Official Datasheet](https://www.eea.europa.eu/data-and-maps/data/airbase-the-european-air-quality-database-6/airbase-products/data/file)
- ğŸ [Python Downloader](https://github.com/JohnPaton/airbase)

### ğŸŒ Global EV Outlook 2025

The Global EV OutlookÂ [1] is an annual report that presents key trends and developments in electric mobility worldwide. It is developed with the support of the Electric Vehicles Initiative (EVI).

For further insights, refer to the dedicated article by Our World in DataÂ [2].

ğŸ§ª Scripts & Notebooks:
- `preprocess/build_ev_share_json.ipynb` â€” Notebook used to clean and process the data into the file electric_car_share_data.json, which is consumed by the web app.

ğŸ“š Citations:
- [1] IEA (2025), Global EV Outlook 2025, IEA, Paris. https://www.iea.org/reports/global-ev-outlook-2025 â€” Licence: CC BY 4.0
- [2] Hannah Ritchie (2024), Tracking Global Data on Electric Vehicles. Published online at OurWorldinData.org. Retrieved from: https://ourworldindata.org/electric-car-sales

## ğŸ“¦ Conda Env

To run the provided Python scripts and notebooks, use the `ee-559` conda environment.
You can create it by running the following command:

```bash
conda env create -f ee-559.yml
```

Once created, activate the environment with:

```bash
conda activate ee-559
```

## ğŸ§± Project Structure

```text
ee-559-deep-learning/
â”‚
â”œâ”€â”€ checkpoints/                       # Trained LoRA checkpoints by model & dataset
â”‚   â”œâ”€â”€ TinyLlama/
â”‚   â”‚   â”œâ”€â”€ dynahate/                  # 3 LoRA-tuned checkpoints (3 seeds) on DynaHate
â”‚   â”‚   â””â”€â”€ hatecheck/                 # 3 LoRA-tuned checkpoints (3 seeds) on HateCheck
â”‚   â”œâ”€â”€ Phi-2/
â”‚   â”‚   â”œâ”€â”€ dynahate/
â”‚   â”‚   â””â”€â”€ hatecheck/
â”‚   â””â”€â”€ opt/
â”‚       â”œâ”€â”€ dynahate/
â”‚       â””â”€â”€ hatecheck/
â”‚
â”œâ”€â”€ data/                              # Raw & preprocessed datasets
â”‚   â”œâ”€â”€ dynahate/
â”‚   â””â”€â”€ hatecheck/
â”‚
â”œâ”€â”€ experiments/                       # YAML configs for each model
â”‚   â”œâ”€â”€ TinyLlama/                     # LoRA & training hyperparameters
â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”œâ”€â”€ phi-2/                         # LoRA & training hyperparameters
â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â””â”€â”€ opt/                           # LoRA & training hyperparameters
â”‚       â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ results/                           # Evaluation outputs & explainability scores
â”‚   â”œâ”€â”€ dynahate/                      # CSVs of accuracy, F1-score, etc.
â”‚   â””â”€â”€ hatecheck/
â”‚
â”œâ”€â”€ utils/                             # Helper modules & scripts
â”‚               
â”œâ”€â”€ requirements.txt                   # Python, PyTorch, Transformers, LoRA, LIMEâ€¦
â”‚
â”œâ”€â”€ run_training_dynahate.sh           # Bash wrapper to train on DynaHate
â”œâ”€â”€ run_training_hatecheck.sh          # Bash wrapper to train on HateCheck
â”‚
â”œâ”€â”€ trainer_dynahate.py                # PyTorch Lightning trainer for DynaHate
â”œâ”€â”€ trainer_hatecheck.py               # PyTorch Lightning trainer for HateCheck
â”‚
â”œâ”€â”€ evaluate_models.py                 
â”‚
â””â”€â”€ compute_lime_scores.py             

```

## ğŸ—‚ï¸ Folder Highlights

- `checkpoints/` â€” LoRAâ€fine-tuned model weights, organized by TinyLlama, Phi-2 and OPT-1.3B, each with DynaHate and HateCheck seeds.

Because the `checkpoints.tar.gz` file is split into parts to comply with the 2 GB Git LFS limit, you can recombine them into a single archive by running:

```bash
cat checkpoints.tar.gz.part-* > checkpoints.tar.gz
```
Once the parts are merged, extract the contents with:
```
tar -xzvf checkpoints.tar.gz
```

- `data/` â€” Raw and preprocessed DynaHate & HateCheck datasets ready for training and evaluation.  
- `experiments/` â€” YAML config files specifying LoRA & training hyperparameters for each model.  
- `results/` â€”  
  - `metrics/`: CSVs with accuracy, F1-score, etc.  
  - `lime/`: Tokenâ€level attribution scores (pre- and post-fine-tuning).  
- `utils/` â€” Helper modules for text dataset creation, visualization, and common utilities.  
- **Rootâ€level scripts**:  
  - `run_training_dynahate.sh` & `run_training_hatecheck.sh`: launch datasetâ€specific training.  
  - `trainer_dynahate.py` & `trainer_hatecheck.py`: PyTorch Lightning training pipelines.  
  - `evaluate_models.py`: evaluate best checkpoints on DynaHate or HateCheck.  
  - `compute_lime_scores.py`: generate LIME explanations for selected models.  
- `requirements.txt` â€” Python, PyTorch, Transformers, LoRA, LIME and related dependencies.  

